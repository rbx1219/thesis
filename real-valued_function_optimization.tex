\chapter{Real-valued function optimization}
\label{ch:real-valued function optimization}

In this chapter, we first briefly introduce basic knowledge for
understanding real-valued problem.
Since this research concentrates on handling real-valued function
optimization, we give a formal definition for real-valued function
optimization and several examples in Section~\ref{sec:problem define}
and then introduce basic knowledge for the optimization problem later in
Section~\ref{sec:problem difficulty}. 


\section{Real-valued problems}
\label{sec:problem define}
A real-valued function $f$ can be defined as follows

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\[f \colon \mathcal{A} \subseteq \mathbb{R}^n \mapsto \mathbb{R} , x \mapsto f(x),\] where $x \in \mathcal{A}$ , $f(x) \in \mathbb{R}$.

Under the definition above, real-valued function optimization is to
maximize or minimize the function value of $f$.
Since minimizing $f(x)$ is equal to maximizing $-f(x)$, we take
minimization as the representation of real-valued optimization in this
thesis, and it can be defined accordingly as \[
\operatornamewithlimits{argmin}\limits_{x} f(x).\]

Typically, $\mathcal{A}$ is some subsets of the Euclidean space
$\mathbb{R}^n$ specified by some constraints that members of
$\mathcal{A}$ have to satisfy such as equality and inequality.
The domain of objective function, $\mathcal{A}$, is called \emph{search
space} and the elements of $\mathcal{A}$ are called \emph{candidate
solutions}.

A \emph{local optimum} is defined as following: \[\exists x_\ell,
\forall x \in \{x\mid \|x-x_\ell\| \leq \delta \},  f(x) \geq
f(x_\ell),\] for some sufficiently small $\delta > 0$. \\
Then $x_\ell$ is referred to as one of local optima of objective function.

A \emph{global optimum} can be similarly defined:
\[\exists x_g \forall x \in \mathcal{A}, f(x) \geq f(x_g).\]
Then $x_g$ is referred to as global optimum.

\section{What makes the function difficult to solve}
\label{sec:problem difficulty}
Recall that the problems we try to solve are considered as black-box problems.
Without any interior information like analytic form, it is unable to
obtain the peak value by mathematical methods.
Besides, several properties of real-valued function make it hard to optimize.
They are listed in and discussed below.

\subsection{Non-Convex}

Suppose we are solving a problem without any interior information.
Once we obtain some fit solutions, we are wondering if there exists an
operator that recombines the solutions in some way to produce a
better solution.
The Non-convex difficulty illustrates that the proper operator is hard
to fetch.
As a result, even we can obtain local optima easily, the local optima
are less important for obtaining the global optimum.
Figure~\ref{fig:nonconvex} illustrates the concept that in operator space, two
solutions in better region sometimes contribute little to form a better
solution

\begin{figure}[]
  \centering
  \includegraphics[scale = 0.4]{Non-Convex.eps}
  \caption{Operator space of non-convex problem}
  \label{fig:nonconvex}
\end{figure}


%Convex function is defined as following:\\ A function $f \colon
%\mathcal{D} \to \mathcal{R}$ satisfy that \[f(t \times x_1+(1-t) \times
%x_2) < t \times f(x_1) + (1-t) \times f(x_2),\] $\forall x_1,x_2 \in
%\mathcal{D},  \forall t \in (0,1)$.\\
%We could tell from the definition that there is only one local optimum
%in a convex function, so this is meanwhile the global optimum.
%Many real-valued functions are multi-modal, which indicates there are
%more than one local optimum, and the functions are therefore not convex.
%The non-convex property makes it hard to distinguish whether the local
%optimum obtained by some procedure is global optimum or not.

\subsection{Dimensionality}

The property of dimensionality points out that in many science problems,
high-dimensional spaces are usually required.
In comparison to three-dimensional space in our daily experience, the
size of the search space increases vastly adding an extra dimension.
This is the `curse of dimensionalityâ€™ brought by Richard
  Bellman~\cite{bellman1956dynamic}.
The example of this term from Richard Bellman is illustrated here:
When sampling 100 points in the one-dimensional domain $[0,1]$, the
average distance between each point is $10^{-2}$.
However, if we sample in a 10-dimensional domain $[ 0 , 1 ]^{10}$, it
requires $\frac{1}{(10^{-2})^{10}}$ = $10^{20}$ points to achieve the
same sampling density.
The vast growth of the search space implies the dataset of proper size
in low-dimensional space will become too sparse in high-dimensional
space.
In real-world case, the dimension of objective function is usually more
than 10, which is referred to as high dimension.

Figure~\ref{fig:dimensionality} gives a simple illustration of
dimensionality.
We have a 1-D function in Figure~\ref{fig:1d}.
By cross-multiplying each hill, we can obtain a 2-D function in
Figure~\ref{fig:2d}.
The contour graph of the 2-D function is shown in
Figure~\ref{fig:2dc}.
Assume there is an optimizer which performs well on hill-climbing.
In other words, it can reach the peak of the hill it resides easily.
In a 1-D function, the probability of sampling around global optimum is
$\frac{1}{3}$.
When it comes to 2-D function, the probability becomes $\frac{1}{3^2}$.
We can imagine if the dimension increases  to 10, the probability of
sampling around global optimum is $\frac{1}{59049}$, which is an
impractical amount for initializing population.
As a result, finding a global optimum from searching nearby is
impossible when the dimension comes high.

\begin{figure}[h]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width = .8\linewidth]{Original.eps}
  \caption{1-D function}  
  \label{fig:1d}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width = .8\textwidth]{2d.eps}
  \caption{2-D function}
  \label{fig:2d}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width = 0.8\textwidth]{2dcontour.eps}
  \caption{Contour graph of 2-D function}
  \label{fig:2dc}
\end{subfigure}
  \caption{Illustration of dimensionality}
  \label{fig:dimensionality}
\end{figure}

\subsection{Non-separable}

Assume the problem we try to optimize is with n decision variables, separable is defined that if there is a partition for cutting n variables in to $\mathcal{P}$, a set of subsets that:\\
$\forall \mathcal{A}, \mathcal{B} \in \mathcal{P}, \forall x, y, x \in \mathcal{A} , y \in \mathcal{B}$, x and y have no dependency on the other.\\
An ideal partition is that:\\
$\forall \mathcal{A} \in \mathcal{P}, \forall x, y \in \mathcal{A}$, x and y depends on the other.\\
The advantage of separable problems is dimensionality reduction.
For instance, to optimize a problem with n = 100 is considerably hard.
If we can decompose the problem into 25 problems with n = 4 each, or
even 50 problems with n = 2 each, the search cost decreases
exponentially.
Unfortunately, general real-valued optimization problems are
non-separable, which means we have to do difficult optimization in
a high dimensional space.

Consider the contour giving in Figure~\ref{fig:2dc}, we can obtain
global optimum by performing search on the x and y axises independently.
Once we perform a rotation so that $x \mapsto f(Rx)$ where $R$ is a
rotation matrix, the contour graph is transformed into
Figure~\ref{fig:2dcr}.
Giving the contour, the dependency cannot be retrieved by performing
search on two axises independently and we can never obtain the best
$(x,y)$ until the angle of rotation is found.
\begin{figure}
  \centering
  \includegraphics[width = .4\textwidth]{rotatedcontour.eps}
  \caption{Rotated contour graph}
  \label{fig:2dcr}
\end{figure}


\subsection{Ruggedness}

In a black-box problem, \emph{hill-climbing} method is usually used to
obtain a local optimum.
Briefly speaking, hill-climbing is a mutation of current best solution
trying to get a better one.
However, in a rugged problem, there are a large amount of local optima
which probably steep between each.
Under the circumstances, it is difficult to climb from one local optimum to another.
Hence, approaches have to be designed to handle the situation that the
candidate solution is easily trapped in local optima.

Considering the 1-D function given in Figure~\ref{fig:1d}, we
perturbate the function by adding random noise every given interval
shown in Figure~\ref{fig:rugged}.
The local optima is much more than 3 which is the number of local optima
in original function.

\begin{figure}[h]
  \centering
  \includegraphics[width = 0.4\textwidth]{Rugged.eps}
  \caption{A rugged environment}
  \label{fig:rugged}
\end{figure}

\subsection{Ill-conditioned}

Condition number is defined to explain how the function value will be
affected by a small perturbation on input.
Typically this is used to measure how sensitive a function is to changes
on the input.
A problem is said to be well-conditioned with a small condition number,
while ill-conditioned problems are the ones with large
condition numbers.
In real-valued functions, being ill-conditioned means the problem has
high curvature and is sensitive to small changes on input.
In real world problems, it is common that the condition number of a
problem is up to $10^{10}$.
Though using information of gradient is enough to solve a
well-conditioned problem, it is unavoidable to use second order
information to conquer a ill-conditioned problem.



To sum up, with the properties described above, the real-world
optimization is almost impossible to handle with a greedy method.
The main reason is the number of local optima is usually large, a greedy
approach is always easily trapped in a local optimum due to the property
of \emph{non-convex} and \emph{ruggedness}.
On the other hand, random search does little effort because of the
property of \emph{dimensionality} makes the search space huge.
Without a heuristic, it is too hard to figure out the `Needle in a
haystack'.

There are many approaches developed to conquer the hazard, and they vary
from eliminating some of the properties in order to generate better
solutions.
Among the methods the property of \emph{ruggedness} is usually
challenged by increasing exploration, and the property of
\emph{non-separable} is usually challenged by estimating the dependency
between decision variables.
In next chapter we will give brief overview to two approaches.
